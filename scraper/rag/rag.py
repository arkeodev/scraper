"""
Module for handling question-answering functionality using ScrapeGraphAI.
"""

import logging
from typing import List, Optional

from scrapegraphai.graphs import BaseGraph
from scrapegraphai.nodes import GenerateAnswerNode

from scraper.errors import QueryError
from scraper.nodes.rag_node import RAGNode


class SgRag:
    """
    Handles the Retrieval-Augmented Generation (RAG) functionality for question answering.
    """

    def __init__(self, documents: List[str], llm, embed_model):
        """
        Initializes the QuestionAnswering instance with a list of documents and model configurations.

        Args:
            documents: A list of documents as strings.
            llm: llm model.
            embed_model: embedding model.
        """
        self.documents = documents
        self.embed_model = embed_model
        self.llm = llm
        self._setup_graph()

    def _setup_graph(self):
        """
        Sets up the graph with the necessary nodes and configurations.
        """
        self.rag_node = RAGNode(
            input="user_prompt & (parsed_doc | doc)",
            output=["relevant_chunks"],
            node_config={
                "llm_model": self.llm,
                "embedder_model": self.embed_model,
                "verbose": True,
            },
        )
        self.generate_answer_node = GenerateAnswerNode(
            input="user_prompt & (relevant_chunks | parsed_doc | doc)",
            output=["answer"],
            node_config={"llm_model": self.llm, "verbose": True},
        )
        self.graph = BaseGraph(
            nodes=[self.rag_node, self.generate_answer_node],
            edges=[(self.rag_node, self.generate_answer_node)],
            entry_point=self.rag_node,
            use_burr=True,
            burr_config={
                "project_name": "universal-scraper",
                "app_instance_id": "001",
            },
        )

    def execute(self, prompt: str) -> Optional[str]:
        """
        Executes the graph to generate an answer for a given prompt.

        Args:
            prompt: The user's query or question as a string.

        Returns:
            The generated answer as a string, or None if no answer is found.
        """
        logging.info(f"Received question: {prompt}")
        try:
            result, execution_info = self.graph.execute(
                {"user_prompt": prompt, "doc": self.documents}
            )
            answer = result.get("answer", {})
            logging.info(answer)
            if isinstance(answer, dict) and answer:
                output = next(iter(answer.values()), None)
                if output:
                    logging.info(f"Generated answer: {output}")
                else:
                    logging.info("Answer dictionary is empty.")
                return output
            else:
                logging.info("No answer generated by the model.")
                return None

        except QueryError as e:
            logging.error(f"Query processing failed: {e}")
            return None
