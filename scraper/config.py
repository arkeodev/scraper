"""
Configuration module for application settings.
"""

from collections import namedtuple
from typing import List, Optional

from pydantic import BaseModel, Field
from pydantic.functional_validators import field_validator

Task = namedtuple(
    "Task",
    ["id", "source_def", "task_def", "allowed_extensions", "is_url", "content_source"],
)

# Define the task instances
tasks = [
    Task(
        id=1,
        source_def="URL",
        task_def=["Chat", "Summarize", "Get Key Points"],
        allowed_extensions=["url"],
        is_url=True,
        content_source="url",
    ),
    Task(
        id=2,
        source_def="PDF file(s)",
        task_def=["Chat", "Summarize", "Get Key Points"],
        allowed_extensions=["pdf"],
        is_url=False,
        content_source="pdf",
    ),
    Task(
        id=3,
        source_def="E-pub file(s)",
        task_def=["Chat", "Summarize", "Get Key Points"],
        allowed_extensions=["epub"],
        is_url=False,
        content_source="e-book",
    ),
]

# Matches the embedding model names with the language
embedding_models_dict = {
    "OpenAI": "text-embedding-ada-002",
}


class LLMConfig(BaseModel):
    model_name: str
    api_key: str = None
    embedding_model_name: str = ""
    temperature: Optional[float] = Field(0.7, ge=0.0, le=1.0)
    max_tokens: Optional[int] = Field(1_000, gt=0)


class AnswerItem(BaseModel):
    text: str = Field(
        description="The actual text of the answer generated by the language model."
    )
    relevance: Optional[float] = Field(
        default=None,
        description="A relevance score between 0 and 1, representing how relevant the language model considers this answer to the question.",
    )

    @field_validator("text")
    def check_text_not_empty(cls, value):
        if not value.strip():
            raise ValueError("Text must not be empty.")
        return value


class AnswerSchema(BaseModel):
    question: str = Field(
        description="The original question posed to the language learning model."
    )
    answers: List[AnswerItem] = Field(
        description="A list of answers provided by the language model, each encapsulated as an AnswerItem."
    )
    source: str = Field(
        description="The original content source of the question (e.g., 'url', 'pdf', 'e-book')."
    )
    success: bool = Field(
        default=True,
        description="Indicates whether the language model successfully generated an answer without errors.",
    )
    error_message: Optional[str] = Field(
        default=None,
        description="Contains an error message if the language model failed to generate a valid answer.",
    )

    @field_validator("answers")
    def check_relevance(cls, item):
        if item.relevance is not None and (item.relevance < 0 or item.relevance > 1):
            raise ValueError("Relevance must be between 0 and 1.")
        return item
