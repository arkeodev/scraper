"""
Module for handling question-answering functionality using ScrapeGraphAI.
"""

import logging
from typing import List, Optional

from langchain_openai import OpenAIEmbeddings
from scrapegraphai.graphs import BaseGraph
from scrapegraphai.models import OpenAI
from scrapegraphai.nodes import GenerateAnswerNode, RAGNode

from scraper.config import LLMConfig
from scraper.errors import QueryError


class SgRag:
    """
    Handles the Retrieval-Augmented Generation (RAG) functionality for question answering.
    """

    def __init__(self, documents: List[str], model_config: LLMConfig):
        """
        Initializes the QuestionAnswering instance with a list of documents and model configurations.

        Args:
            documents: A list of documents as strings.
            model_config: Model configuration parameters.
        """
        self.documents = documents
        self.embed_model = OpenAIEmbeddings(api_key=model_config.api_key)
        self.llm = OpenAI(
            llm_config={
                "model_name": model_config.llm_model_name,
                "openai_api_key": model_config.api_key,
                "max_tokens": model_config.max_tokens,
                "temperature": model_config.temperature,
            }
        )

    def rag(self, prompt: str) -> Optional[str]:
        """
        Generates an answer for a given prompt using RAG functionality.

        Args:
            prompt: The user's query or question as a string.

        Returns:
            The generated answer as a string, or None if no answer is found.
        """
        logging.info(f"Received question: {prompt}")

        try:
            # Define the graph nodes with configurations
            rag_node = RAGNode(
                input="user_prompt & (parsed_doc | doc)",
                output=["relevant_chunks"],
                node_config={
                    "llm_model": self.llm,
                    "embedder_model": self.embed_model,
                    "verbose": True,
                },
            )
            generate_answer_node = GenerateAnswerNode(
                input="user_prompt & (relevant_chunks | parsed_doc | doc)",
                output=["answer"],
                node_config={"llm_model": self.llm, "verbose": True},
            )

            # Construct the graph
            graph = BaseGraph(
                nodes=[rag_node, generate_answer_node],
                edges=[(rag_node, generate_answer_node)],
                entry_point=rag_node,
            )

            # Execute the graph
            result, execution_info = graph.execute(
                {"user_prompt": prompt, "parsed_doc": self.documents}
            )
            answer = result.get("answer", {}).get("answer")

            if answer:
                logging.info(f"Generated response: {answer}")
                return answer
            else:
                logging.warning("No answer generated by the model.")
                return None

        except QueryError as e:
            logging.error(f"Query processing failed: {e}")
            return f"Error: {e}"
