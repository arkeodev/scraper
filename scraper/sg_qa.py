"""
Module for handling question-answering functionality using ScrapeGraphAI.
"""

import logging
from typing import List, Optional

from langchain_openai import OpenAIEmbeddings
from scrapegraphai.graphs import BaseGraph
from scrapegraphai.models import OpenAI
from scrapegraphai.nodes import GenerateAnswerNode, RAGNode

from scraper.config import LLMConfig
from scraper.errors import QueryError
from scraper.interfaces import Rag


class SgRag(Rag):
    """
    Handles the Retrieval-Augmented Generation (RAG) functionality for question answering.
    """

    def __init__(self, documents: List[str], model_config: LLMConfig):
        """
        Initializes the QuestionAnswering instance with a list of documents and model configurations.

        Args:
            documents: A list of documents as strings.
            model_config: Model configuration parameters.
        """
        self.documents = documents
        self.embed_model = OpenAIEmbeddings(api_key=model_config.api_key)
        self.llm = OpenAI(
            llm_config={
                "model_name": model_config.llm_model_name,
                "openai_api_key": model_config.api_key,
                "max_tokens": model_config.max_tokens,
                "temperature": model_config.temperature,
            }
        )
        self._setup_graph()

    def _setup_graph(self):
        """
        Sets up the graph with the necessary nodes and configurations.
        """
        self.rag_node = RAGNode(
            input="user_prompt & (parsed_doc | doc)",
            output=["relevant_chunks"],
            node_config={
                "llm_model": self.llm,
                "embedder_model": self.embed_model,
                "verbose": True,
            },
        )
        self.generate_answer_node = GenerateAnswerNode(
            input="user_prompt & (relevant_chunks | parsed_doc | doc)",
            output=["answer"],
            node_config={"llm_model": self.llm, "verbose": True},
        )
        self.graph = BaseGraph(
            nodes=[self.rag_node, self.generate_answer_node],
            edges=[(self.rag_node, self.generate_answer_node)],
            entry_point=self.rag_node,
        )

    def execute(self, prompt: str) -> Optional[str]:
        """
        Executes the graph to generate an answer for a given prompt.

        Args:
            prompt: The user's query or question as a string.

        Returns:
            The generated answer as a string, or None if no answer is found.
        """
        logging.info(f"Received question: {prompt}")
        try:
            result, execution_info = self.graph.execute(
                {"user_prompt": prompt, "parsed_doc": self.documents}
            )
            answer = result.get("answer", {})
            if isinstance(answer, dict) and answer:
                output = next(iter(answer.values()), None)
                if output:
                    logging.info(f"Generated answer: {output}")
                else:
                    logging.info("Answer dictionary is empty.")
                return output
            else:
                logging.info("No answer generated by the model.")
                return None

        except QueryError as e:
            logging.error(f"Query processing failed: {e}")
            return None
